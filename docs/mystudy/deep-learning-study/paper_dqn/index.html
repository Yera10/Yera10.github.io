<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Playing Atari with Deep Reinforcement Learning # paper (2013)
Abstract # We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.
The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. 모델은 Q-learing으로 변형 학습된 CNN인데, raw pixel을 입력으로 받고, 미래 보상을 추정하는 value function 이다."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="DQN 논문"><meta property="og:description" content="Playing Atari with Deep Reinforcement Learning # paper (2013)
Abstract # We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.
The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. 모델은 Q-learing으로 변형 학습된 CNN인데, raw pixel을 입력으로 받고, 미래 보상을 추정하는 value function 이다."><meta property="og:type" content="article"><meta property="og:url" content="https://yera10.github.io/docs/mystudy/deep-learning-study/paper_dqn/"><meta property="article:section" content="docs"><title>DQN 논문 | 이세상의 모든 노트</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=stylesheet href=/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz+OQteg=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.6a6d4ca0e03c96f822747869a4c424cdf27b1cdeb53dcca24c44cb80b0e1a23d.js integrity="sha256-am1MoOA8lvgidHhppMQkzfJ7HN61PcyiTETLgLDhoj0=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-B1LML6H5K9"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B1LML6H5K9",{anonymize_ip:!1})}</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>이세상의 모든 노트</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li class=book-section-flat><a href=/docs/mystudy/>My Study</a><ul><li><input type=checkbox id=section-472e96a9c612ee135e9f631f838b4095 class=toggle checked>
<label for=section-472e96a9c612ee135e9f631f838b4095 class="flex justify-between"><a href=/docs/mystudy/deep-learning-study/>딥러닝</a></label><ul><li><a href=/docs/mystudy/deep-learning-study/historical_review/>Historical Review</a></li><li><a href=/docs/mystudy/deep-learning-study/paper_alexnet/>AlexNet 논문</a></li><li><a href=/docs/mystudy/deep-learning-study/alexnet-in-pytorch/>AlexNet 구현 (Pytorch)</a></li><li><a href=/docs/mystudy/deep-learning-study/paper_dqn/ class=active>DQN 논문</a></li><li><a href=/docs/mystudy/deep-learning-study/dl_aimath/>AI, Math</a></li><li><a href=/docs/mystudy/deep-learning-study/dl_pytorch/>PyTorch</a></li><li><a href=/docs/mystudy/deep-learning-study/outputsize/>Output Size 계산</a></li><li><a href=/docs/mystudy/deep-learning-study/paper_lora/>Paper Lo Ra</a></li></ul></li><li><input type=checkbox id=section-43609ff8ade5f49a9a42f6c75dfa8b7c class=toggle>
<label for=section-43609ff8ade5f49a9a42f6c75dfa8b7c class="flex justify-between"><a href=/docs/mystudy/algorithm-note/>Algorithm Note</a></label><ul><li><input type=checkbox id=section-87f5a5aba3a2eaef9e324cd61b28b682 class=toggle>
<label for=section-87f5a5aba3a2eaef9e324cd61b28b682 class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part2/>Part 2</a></label><ul><li><input type=checkbox id=section-9ecde276efb9d73a10e822395ea24222 class=toggle>
<label for=section-9ecde276efb9d73a10e822395ea24222 class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part2/3_%EA%B7%B8%EB%A6%AC%EB%94%94/>3. 그리디 알고리즘</a></label><ul><li><a href=/docs/mystudy/algorithm-note/part2/3_%EA%B7%B8%EB%A6%AC%EB%94%94/1%EC%9D%B4%EB%90%A0%EB%95%8C%EA%B9%8C%EC%A7%80/>1이될때까지</a></li><li><a href=/docs/mystudy/algorithm-note/part2/3_%EA%B7%B8%EB%A6%AC%EB%94%94/%EC%88%AB%EC%9E%90%EC%B9%B4%EB%93%9C%EA%B2%8C%EC%9E%84/>숫자카드게임</a></li><li><a href=/docs/mystudy/algorithm-note/part2/3_%EA%B7%B8%EB%A6%AC%EB%94%94/%ED%81%B0%EC%88%98%EC%9D%98%EB%B2%95%EC%B9%99/>큰수의법칙</a></li></ul></li><li><input type=checkbox id=section-d6c984057099e052e66a4f1aa50e7f0c class=toggle>
<label for=section-d6c984057099e052e66a4f1aa50e7f0c class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part2/4_%EA%B5%AC%ED%98%84/>4. 구현</a></label><ul><li><a href=/docs/mystudy/algorithm-note/part2/4_%EA%B5%AC%ED%98%84/%EA%B2%8C%EC%9E%84-%EA%B0%9C%EB%B0%9C/>게임 개발</a></li><li><a href=/docs/mystudy/algorithm-note/part2/4_%EA%B5%AC%ED%98%84/%EC%83%81%ED%95%98%EC%A2%8C%EC%9A%B0/>상하좌우</a></li><li><a href=/docs/mystudy/algorithm-note/part2/4_%EA%B5%AC%ED%98%84/%EC%8B%9C%EA%B0%81/>시각</a></li><li><a href=/docs/mystudy/algorithm-note/part2/4_%EA%B5%AC%ED%98%84/%EC%99%95%EC%8B%A4%EC%9D%98%EB%82%98%EC%9D%B4%ED%8A%B8/>왕실의나이트</a></li></ul></li><li><input type=checkbox id=section-32c1b3fcaa327485a5a7afbcc8ea04d2 class=toggle>
<label for=section-32c1b3fcaa327485a5a7afbcc8ea04d2 class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part2/5_dfs_bfs/>5. DFS & BFS</a></label><ul><li><a href=/docs/mystudy/algorithm-note/part2/5_dfs_bfs/%EB%AF%B8%EB%A1%9C%ED%83%88%EC%B6%9C/>미로탈출</a></li><li><a href=/docs/mystudy/algorithm-note/part2/5_dfs_bfs/%EC%9D%8C%EB%A3%8C%EC%88%98-%EC%96%BC%EB%A0%A4-%EB%A8%B9%EA%B8%B0/>음료수 얼려 먹기</a></li><li><a href=/docs/mystudy/algorithm-note/part2/5_dfs_bfs/datastructure/>자료구조 기초</a></li></ul></li><li><input type=checkbox id=section-ec2ec8f1f45d90b15ad9249032dc4793 class=toggle>
<label for=section-ec2ec8f1f45d90b15ad9249032dc4793 class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part2/6_%EC%A0%95%EB%A0%AC/>6. 정렬</a></label><ul><li><a href=/docs/mystudy/algorithm-note/part2/6_%EC%A0%95%EB%A0%AC/%EC%84%B1%EC%A0%81%EC%9D%B4%EB%82%AE%EC%9D%80%EC%88%9C%EC%84%9C%EB%A1%9C/>성적이낮은순서로</a></li><li><a href=/docs/mystudy/algorithm-note/part2/6_%EC%A0%95%EB%A0%AC/%EC%9C%84%EC%97%90%EC%84%9C%EC%95%84%EB%9E%98%EB%A1%9C/>위에서아래로</a></li><li><a href=/docs/mystudy/algorithm-note/part2/6_%EC%A0%95%EB%A0%AC/%EC%A0%95%EB%A0%AC%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/%EA%B3%84%EC%88%98%EC%A0%95%EB%A0%AC/>계수정렬</a></li><li><a href=/docs/mystudy/algorithm-note/part2/6_%EC%A0%95%EB%A0%AC/%EC%A0%95%EB%A0%AC%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/%EC%82%BD%EC%9E%85%EC%A0%95%EB%A0%AC/>삽입정렬</a></li><li><a href=/docs/mystudy/algorithm-note/part2/6_%EC%A0%95%EB%A0%AC/%EC%A0%95%EB%A0%AC%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/%EC%84%A0%ED%83%9D%EC%A0%95%EB%A0%AC/>선택정렬</a></li><li><a href=/docs/mystudy/algorithm-note/part2/6_%EC%A0%95%EB%A0%AC/%EC%A0%95%EB%A0%AC%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/%ED%80%B5%EC%A0%95%EB%A0%AC/>퀵정렬</a></li></ul></li><li><input type=checkbox id=section-4118e2eceb3f1bb89e7573c149cbccd8 class=toggle>
<label for=section-4118e2eceb3f1bb89e7573c149cbccd8 class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part2/7_%EC%9D%B4%EC%A7%84%ED%83%90%EC%83%89/>7. 이진탐색</a></label><ul><li><a href=/docs/mystudy/algorithm-note/part2/7_%EC%9D%B4%EC%A7%84%ED%83%90%EC%83%89/%EB%96%A1%EB%B3%B6%EC%9D%B4%EB%96%A1%EB%A7%8C%EB%93%A4%EA%B8%B0/>떡볶이떡만들기</a></li><li><a href=/docs/mystudy/algorithm-note/part2/7_%EC%9D%B4%EC%A7%84%ED%83%90%EC%83%89/%EB%B6%80%ED%92%88%EC%B0%BE%EA%B8%B0/>부품찾기</a></li><li><a href=/docs/mystudy/algorithm-note/part2/7_%EC%9D%B4%EC%A7%84%ED%83%90%EC%83%89/%EC%A7%95%EA%B2%80%EB%8B%A4%EB%A6%AC%EA%B1%B4%EB%84%88%EA%B8%B0/>징검다리건너기</a></li></ul></li><li><input type=checkbox id=section-0a6b47a1b1b81880d41ff66826f8e9d7 class=toggle>
<label for=section-0a6b47a1b1b81880d41ff66826f8e9d7 class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part2/8_%EB%8B%A4%EC%9D%B4%EB%82%98%EB%AF%B9%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/>8. DP</a></label><ul><li><a href=/docs/mystudy/algorithm-note/part2/8_%EB%8B%A4%EC%9D%B4%EB%82%98%EB%AF%B9%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/1%EB%A1%9C%EB%A7%8C%EB%93%A4%EA%B8%B0/>1로만들기</a></li><li><a href=/docs/mystudy/algorithm-note/part2/8_%EB%8B%A4%EC%9D%B4%EB%82%98%EB%AF%B9%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EA%B0%9C%EB%AF%B8%EC%A0%84%EC%82%AC/>개미전사</a></li><li><a href=/docs/mystudy/algorithm-note/part2/8_%EB%8B%A4%EC%9D%B4%EB%82%98%EB%AF%B9%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B0%94%EB%8B%A5%EA%B3%B5%EC%82%AC/>바닥공사</a></li><li><a href=/docs/mystudy/algorithm-note/part2/8_%EB%8B%A4%EC%9D%B4%EB%82%98%EB%AF%B9%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B8%ED%99%94%ED%8F%90%EA%B5%AC%EC%84%B1/>효율적인화폐구성</a></li></ul></li><li><input type=checkbox id=section-1662a2a656f5d3d320218332170be07a class=toggle>
<label for=section-1662a2a656f5d3d320218332170be07a class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part2/9_%EC%B5%9C%EB%8B%A8%EA%B2%BD%EB%A1%9C/>9. 최단경로</a></label><ul></ul></li></ul></li><li><input type=checkbox id=section-90ac3913cacd642470f02c4e44c235cb class=toggle>
<label for=section-90ac3913cacd642470f02c4e44c235cb class="flex justify-between"><a href=/docs/mystudy/algorithm-note/part3/>Part 3</a></label><ul><li><input type=checkbox id=section-bf3c756a96c759cc7852b57440790b7f class=toggle>
<label for=section-bf3c756a96c759cc7852b57440790b7f class="flex justify-between"><a role=button>그리디 알고리즘</a></label><ul><li><a href=/docs/mystudy/algorithm-note/part3/11_greedy/%EA%B3%B1%ED%95%98%EA%B8%B0-%ED%98%B9%EC%9D%80-%EB%8D%94%ED%95%98%EA%B8%B0/>곱하기 혹은 더하기</a></li><li><a href=/docs/mystudy/algorithm-note/part3/11_greedy/%EB%A7%8C%EB%93%A4%EC%88%98%EC%97%86%EB%8A%94%EA%B8%88%EC%95%A1/>만들수없는금액</a></li><li><a href=/docs/mystudy/algorithm-note/part3/11_greedy/%EB%AA%A8%ED%97%98%EA%B0%80%EA%B8%B8%EB%93%9C/>모험가길드</a></li><li><a href=/docs/mystudy/algorithm-note/part3/11_greedy/%EB%B3%BC%EB%A7%81%EA%B3%B5%EA%B3%A0%EB%A5%B4%EA%B8%B0/>볼링공고르기</a></li></ul></li></ul></li><li><a href=/docs/mystudy/algorithm-note/data_structure/>데이터 구조</a></li><li><a href=/docs/mystudy/algorithm-note/prime_number_code/>소수 관련 코드</a></li></ul></li><li><input type=checkbox id=section-d0422ed221db5f77315362ccbc507475 class=toggle>
<label for=section-d0422ed221db5f77315362ccbc507475 class="flex justify-between"><a href=/docs/mystudy/hugo-blog/>Hugo로 github 블로그 만들기</a></label><ul><li><a href=/docs/mystudy/hugo-blog/1_%EC%84%A4%EC%B9%98/>셋팅 및 시작</a></li><li><a href=/docs/mystudy/hugo-blog/2_%EC%BB%A8%ED%85%90%EC%B8%A0%EC%97%B0%EA%B2%B0/>컨텐츠 연결</a></li><li><a href=/docs/mystudy/hugo-blog/3_%EC%9E%90%EB%8F%99%EC%97%85%EB%A1%9C%EB%93%9C/>자동 업로드</a></li></ul></li></ul></li><li class=book-section-flat><a href=/docs/memo/>메모장</a><ul><li><input type=checkbox id=section-dbf12aea4ba6ece0bc58517be081ff61 class=toggle>
<label for=section-dbf12aea4ba6ece0bc58517be081ff61 class="flex justify-between"><a href=/docs/memo/pytorch-study/>Pytorch</a></label><ul><li><a href=/docs/memo/pytorch-study/nn/>Nn</a></li><li><input type=checkbox id=section-d05407db8bfeedc9a29e39a783a69ac5 class=toggle>
<label for=section-d05407db8bfeedc9a29e39a783a69ac5 class="flex justify-between"><a href=/docs/memo/pytorch-study/dataset_module/>파이토치 데이터 모듈</a></label></li><li><input type=checkbox id=section-860b30dc364c1b43f944597b2d7cc950 class=toggle>
<label for=section-860b30dc364c1b43f944597b2d7cc950 class="flex justify-between"><a href=/docs/memo/pytorch-study/tensor/>파이토치 텐서?</a></label></li></ul></li><li><input type=checkbox id=section-73671f80b377e01c565aa90aa7ee004a class=toggle>
<label for=section-73671f80b377e01c565aa90aa7ee004a class="flex justify-between"><a href=/docs/memo/commands/>자주 쓰는 명령어들</a></label><ul><li><input type=checkbox id=section-e603ea93b4c5641d8f935bf5a00859b8 class=toggle>
<label for=section-e603ea93b4c5641d8f935bf5a00859b8 class="flex justify-between"><a href=/docs/memo/commands/conda/>Conda</a></label><ul><li><a href=/docs/memo/commands/conda/virtual_env/>가상환경 관련</a></li></ul></li><li><input type=checkbox id=section-af0bdbd2a00e3e0b212a55987223ffc8 class=toggle>
<label for=section-af0bdbd2a00e3e0b212a55987223ffc8 class="flex justify-between"><a href=/docs/memo/commands/etc/>Etc</a></label><ul><li><a href=/docs/memo/commands/etc/docker/>Docker</a></li><li><a href=/docs/memo/commands/etc/python-parameter/>python 실행파일 인자값 받기</a></li><li><a href=/docs/memo/commands/etc/python-venv/>python3 venv 사용법</a></li><li><a href=/docs/memo/commands/etc/python-server/>python3 서버열기</a></li><li><a href=/docs/memo/commands/etc/vscode_shortcuts/>VSCode 단축키</a></li><li><a href=/docs/memo/commands/etc/terminal_theme/>터미널 테마</a></li></ul></li><li><input type=checkbox id=section-45cef228deb80bf75e3df458f39dc21a class=toggle>
<label for=section-45cef228deb80bf75e3df458f39dc21a class="flex justify-between"><a href=/docs/memo/commands/linux/>Linux</a></label><ul><li><a href=/docs/memo/commands/linux/gpu/>GPU 관련</a></li><li><a href=/docs/memo/commands/linux/screen/>Ubuntu Screen 명령어</a></li><li><a href=/docs/memo/commands/linux/background/>백그라운드 관련</a></li><li><a href=/docs/memo/commands/linux/zip_tar_gz/>압축 관련</a></li><li><a href=/docs/memo/commands/linux/file_dir/>파일 생성/이동/복사/삭제</a></li></ul></li><li><input type=checkbox id=section-8c56b6f03dacfd96ad3b5d449acaea30 class=toggle>
<label for=section-8c56b6f03dacfd96ad3b5d449acaea30 class="flex justify-between"><a href=/docs/memo/commands/poetry/>Poetry</a></label><ul><li><a href=/docs/memo/commands/poetry/poetry_start/>Poetry 시작하기</a></li><li><a href=/docs/memo/commands/poetry/virtual_env/>가상환경 관련</a></li><li><a href=/docs/memo/commands/poetry/export/>내보내기 (requirements.txt)</a></li><li><a href=/docs/memo/commands/poetry/dependency/>의존성 관련</a></li></ul></li><li><input type=checkbox id=section-d62e43e08089551fa5c3188ab2ea83ba class=toggle>
<label for=section-d62e43e08089551fa5c3188ab2ea83ba class="flex justify-between"><a href=/docs/memo/commands/git/>Git 관련</a></label><ul><li><a href=/docs/memo/commands/git/clone/>clone 관련</a></li><li><a href=/docs/memo/commands/git/cancel/>commit 취소</a></li><li><a href=/docs/memo/commands/git/credential/>credential 관련</a></li><li><a href=/docs/memo/commands/git/submodule/>서브모듈 관련</a></li><li><a href=/docs/memo/commands/git/initial_setup/>초기설정 관련</a></li></ul></li></ul></li><li><input type=checkbox id=section-232998b4ddb91b0de578d4b83588e89e class=toggle>
<label for=section-232998b4ddb91b0de578d4b83588e89e class="flex justify-between"><a href=/docs/memo/freq-used-code/>자주 쓰는 코드들</a></label><ul><li><a href=/docs/memo/freq-used-code/histogram/>간단한 히스토그램 python</a></li><li><a href=/docs/memo/freq-used-code/warnings/>경고 무시 python</a></li><li><a href=/docs/memo/freq-used-code/time/>시간 측정 python</a></li><li><a href=/docs/memo/freq-used-code/file_zip/>파일 압축 python</a></li><li><a href=/docs/memo/freq-used-code/file_open/>파일 읽기 python</a></li></ul></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>DQN 논문</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#background>Background</a></li></ul></nav></aside></header><article class=markdown><h1 id=playing-atari-with-deep-reinforcement-learning>Playing Atari with Deep Reinforcement Learning
<a class=anchor href=#playing-atari-with-deep-reinforcement-learning>#</a></h1><p><a href=https://arxiv.org/abs/1312.5602>paper</a> (2013)</p><h2 id=abstract>Abstract
<a class=anchor href=#abstract>#</a></h2><p>We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.</p><p>The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.<br>모델은 Q-learing으로 변형 학습된 CNN인데,
raw pixel을 입력으로 받고, 미래 보상을 추정하는 value function 이다.</p><p>We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm.<br>학습 알고리즘이나 아키텍처를 조정하지 않고, 적용해보았다.</p><p>We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.<br>6 게임에서 모든 이전 접근을 능가했으며, 그 중 세 게임은 전문 인간을 능가했다.</p><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>Learning to control agents directly from high-dimensional sensory inputs like vision and speech is one of the long-standing challenges of reinforcement learning (RL).<br>시각, 음성과 같은 고차원 감각 입력으로부터 agent를 직접 제어하는 학습은 강화학습에서 오랜 과제였다.</p><p>Most successful RL applications that operate on these domains have relied on hand-crafted features combined with linear value functions or policy representations.<br>이러한 도메인에서 성공적인 RL 애플리케이션의 대부분은 선형 value function과 policy 표현들이 결합된 수작업한 feature에 의존해왔다.</p><p>Clearly, the performance of such systems heavily relies on the quality of the feature representation.
그러한 시스템의 성능은 분명히 feature 표현에 크게 의존한다.</p><p>Recent advances in deep learning have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision and speech recognition.
최근 딥러닝의 발전으로 원시 감각 데이터에서 높은 수준의 feature를 추출할 수 있게 되어, 컴퓨터 비전과 음성 인식에서 획기전인 발전을 이루었다.</p><p>These methods utilise a range of neural network architectures, including convolutional networks, multilayer perceptrons, restricted Boltzmann machines and recurrent neural networks, and have exploited both supervised and unsupervised learning.<br>이 방법들은 다양한 신경망 구조(CNN, 다층 퍼셉트론, 볼츠만 머신, RNN)를 활용하며, 지도학습과 비지도 학습을 모두 활용.</p><p>It seems natural to ask whether similar techniques could also be beneficial for RL with sensory data.<br>비슷한 기술들이 감각데이터를 활용한 RL에도 도움이 되는지 묻는 것은 자연스럽다.</p><p>However reinforcement learning presents several challenges from a deep learning perspective.<br>하지만 강화학습은 딥러닝 관점에서 몇 가지 어려움이 있다.</p><p>Firstly, most successful deep learning applications to date have required large amounts of handlabelled training data.<br>첫째로, 성공정인 딥러닝 애플리케이션들의 대부분은 handlabbelling된 많은 양의 훈련데이터가 있었다.</p><p>RL algorithms, on the other hand, must be able to learn from a scalar reward signal that is frequently sparse, noisy and delayed.<br>반면, RL 알고리즘은 거의 희박하고, 잡음이 많고, 지연 있는 scalar 보상 신호로부터 학습할 수 있어야 한다.</p><p>The delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning.<br>action과 reward 사이의 지연을 지도학습에서 입력과 target 사이에 직접적인 연관성과 비교하면 특히 어려워 보인다.</p><p>Another issue is that most deep learning algorithms assume the data samples to be independent, while in reinforcement learning one typically encounters sequences of highly correlated states.<br>또 다른 문제로는 대부분 딥러닝 알고리즘은 데이터의 모든 샘플들이 독립적이라고 가정하지만, 강화학습에서는 상관관계가 높은 state들의 시퀀스를 접한다는 것이다.</p><p>Furthermore, in RL the data distribution changes as the algorithm learns new behaviours, which can be problematic for deep learning methods that assume a fixed underlying distribution.<br>또한, 강화학습에서는 데이터 분포가 알고리즘이 새로운 행동을 배우면서 변한다. 이것은 기본 고정 분포를 가정하는 딥러닝에서는 문제가 될 수 있다.</p><p>This paper demonstrates that a convolutional neural network can overcome these challenges to learn successful control policies from raw video data in complex RL environments.
본 논문은 CNN이 복잡한 RL 환경에서 원시 영상 데이터로부터 성공적인 제어 정책(policy)을 학습하는 과제를 극복할 수 있다는 것을 보여준다.</p><p>The network is trained with a variant of the Q-learning algorithm, with stochastic gradient descent to update the weights.
네트워크는 변형된 Q-learning 알고리즘과 확률적 경사하강법으로 학습되었다.</p><p>To alleviate the problems of correlated data and non-stationary distributions, we use an experience replay mechanism which randomly samples previous transitions, and thereby smooths the training distribution over many past behaviors.
상관 데이터와 비정상(non-stationary) 분포 문제를 완화하기 위해, 이전 전환을 랜덤하게 샘플링하고, 많은 과거 동작에 대한 분포를 매끄럽게 하는 experience replay 메커니즘을 사용한다.</p><p>We apply our approach to a range of Atari 2600 games implemented in The Arcade Learning Environment (ALE).</p><p>Atari 2600 is a challenging RL testbed that presents agents with a high dimensional visual input (210 × 160 RGB video at 60Hz) and a diverse and interesting set of tasks that were designed to be difficult for humans players.</p><p>Our goal is to create a single neural network agent that is able to successfully learn to play as many of the games as possible.</p><p>The network was not provided with any game-specific information or hand-designed visual features, and was not privy to the internal state of the emulator; it learned from nothing but the video input, the reward and terminal signals, and the set of possible actions—just as a human player would.</p><p>Furthermore the network architecture and all hyperparameters used for training were kept constant across the games.</p><p>So far the network has outperformed all previous RL algorithms on six of the seven games we have attempted and surpassed an expert human player on three of them.</p><p>Figure 1 provides sample screenshots from five of the games used for training.</p><h2 id=background>Background
<a class=anchor href=#background>#</a></h2><p>We consider tasks in which an agent interacts with an environment E, in this case the Atari emulator, in a sequence of actions, observations and rewards.<br>에이전트가 환경 E, 일련의 action들, 관찰, 보상들과 상호작용하는 작업을 고려한다.</p><p>At each time-step the agent selects an action at from the set of legal game actions, A = {1, . . . , K}.<br>매 step마다 에이전트는 가능한 action 집합들(예: A={1, &mldr;, K})로부터 하나의 action을 선택한다.</p><p>The action is passed to the emulator and modifies its internal state and the game score.<br>그 action은 emulator에 전달되고, 내부 상태(state)와 게임 점수를 수정한다.</p><p>In general E may be stochastic. The emulator’s internal state is not observed by the agent; instead it observes an image
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(x_t ∈ R^d\)
</span>from the emulator, which is a vector of raw pixel values representing the current screen.<br>agent는 emulator의 내부 상태를 관찰하지는 않지만, 현재 스크린에 표현되는 원시 픽셀 벡터의 이미지<span>
\(x_t\)
</span>를 관찰한다.</p><p>In addition it receives a reward <span>\(r_t\)
</span>representing the change in game score.
또한, 게임점수의 변화를 나타내는 보상 <span>\(r_t\)
</span>를 받는다.</p><p>Note that in general the game score may depend on the whole prior sequence of actions and observations; feedback about an action may only be received after many thousands of time-steps have elapsed.<br>일반적으로 게임 점수는 이전 action과 관찰의 전체 시퀀스에 따라 달라질 수 있으며, action에 대한 피드백(보상)은 수천번의 스텝이 경과한 후에 받을 수도 있다.</p><p>Since the agent only observes images of the current screen, the task is partially observed and many emulator states are perceptually aliased, i.e. it is impossible to fully understand the current situation from only the current screen <span>\(x_t\)
</span>.<br>즉, 현재 화면 <span>\(x_t\)
</span>으로부터 현재 상황을 완전히 이해할 수 없다.</p><p>We therefore consider sequences of actions and observations, <span>\(s_t = x_1, a_1, x_2, ..., a_{t−1}, x_t,\)
</span>and learn game strategies that depend upon these sequences.<br>따라서 action과 관찰의 시퀀스 <span>\(s_t = x_1, a_1, x_2, ..., a_{t−1}, x_t,\)
</span>를 고려하여 이 시퀀스에 의존하는 게임전략을 학습한다.</p><p>All sequences in the emulator are assumed to terminate in a finite number of time-steps.<br>에뮬레이터의 모든 시퀀스는 제한된 수의 시간 단계에서 종료되는 것으로 가정됩니다.</p><p>This formalism gives rise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state.<br>이 형식주의는 MDP(Markov Decision Process)를 발생시킨다.</p><p>As a result, we can apply standard reinforcement learning methods for MDPs, simply by using the complete sequence <span>\(s_t\)
</span>as the state representation at time <span>\(t\)
</span>.<br>결과적으로, 시간 t에서의 상태 표현으로서 s_t를 사용하여 MDP에 대한 표준 강화학습법을 적용할 수 있다.</p><p>The goal of the agent is to interact with the emulator by selecting actions in a way that maximises future rewards.<br>에이전트의 목표는 미래 보상을 극대화하는 방식으로 action을 선택하여 emulator와 상호작용하는 것이다.</p><p>We make the standard assumption that future rewards are discounted by a factor of γ per time-step, and define the future discounted return at time t as <span>\(R_t = \sum_{t'=t}^Tγ^{t'-t}r_{t'}\)
</span>, where T is the time-step at which the game terminates.<br>미래 보상이 시간단계당 γ로 할인된다고 가정하고, 시간 t에서의 미래 할인 반환값을 R_t로 정의했다. (T는 게임이 종료되는 시간 단계.)</p><p>We define the optimal action-value function Q∗(s, a) as the maximum expected return achievable by following any strategy, after seeing some sequence s and then taking some action a, <span>\(Q^∗(s, a) = max_π E [R_t|s_t=s, a_t=a, π]\)
</span>, where π is a policy mapping sequences to actions (or distributions over actions).<br>시퀀스 s를 보고 action a를 수행한 후, 모든 전략에 따라 달성할 수 있는 기대 보상이 최대값으로서 최적의 action-value fuction Q*(s, a)을 정의한다.
π는 시퀀스를 action에 매핑하는 정책이다.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#background>Background</a></li></ul></nav></div></aside></main></body></html>