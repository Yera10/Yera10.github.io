<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>My Study on 이세상의 모든 노트</title><link>https://yera10.github.io/docs/mystudy/</link><description>Recent content in My Study on 이세상의 모든 노트</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://yera10.github.io/docs/mystudy/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://yera10.github.io/docs/mystudy/deep-learning-study/alexnet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yera10.github.io/docs/mystudy/deep-learning-study/alexnet/</guid><description>AlexNet # 논문 요약 정리 논문
Abstract # ImageNet LSVRC-2010 (1,200,000장의 고해상도 이미지들)를 1000개의 클래스로 분류하는 CNN 모델 기존 SOTA보다 향상된 성능 6천만개의 파라미터, 65만개의 뉴런으로 이루어져있고, 5개의 Conv-layer와 3개의 FC-layer로 구성되어있다. (최종은 1000-way softmax) 빠르게 훈련시키기 위해 비포화 뉴런과 GPU 구현을 사용 FC-layer에서 오버피팅을 줄이기 위해 dropout 방법 적용 또한, Alexnet의 변형은 ILSVRC-2012에서 오류율 15.3%로 우승한 딥러닝 신경망 아키텍처이다. Introduction # 이 논문의 기여도 ImageNet데이터로 훈련한 CNN모델로 이 데이터셋에 대한 최고의 성능에 달성 2D conv에 최적화된 GPU 구현과 공개 성능을 향상시키고 훈련시간을 단축시키는 몇 가지 특징들이 있다 (Section 3) 오버피팅을 방지하는 몇 가지 기술들 (Section 4) 이당시 2개의 GTX 580 3GB GPUs로 훈련했을 때, 5~6일 소요됐다.</description></item><item><title/><link>https://yera10.github.io/docs/mystudy/deep-learning-study/historical_review/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yera10.github.io/docs/mystudy/deep-learning-study/historical_review/</guid><description>Historical Review # 2012 - AlexNet (패러다임 쇼크!) 2013 - DQN (알파고를 만든 알고리즘) 2014 - Encoder/Decoder, Adam 2015 - GAN, ResNet 2016 - 2017 - Transformer (Attention Is All You Need) 2018 - Bert 2019 - Big Language Models (GPT-X) 2020 - Self-Supervised Learning (SimCLR) Further Reading and Reference
AlexNet (2012) # AlexNet은 딥러닝과 AI 연구의 붐을 일으킨 알고리즘으로 여겨졌다 Yann LeCun이 개발한 초기 LeNet에 기초한 CNN이다 알고리즘을 발전시키고, GPU 성능을 통해 기존 방법들 보다 ImageNet 데이터를 분류하는데 훨씬 뛰어난 성능을 보였다.</description></item><item><title/><link>https://yera10.github.io/docs/mystudy/deep-learning-study/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yera10.github.io/docs/mystudy/deep-learning-study/readme/</guid><description>DL # Deep Learning 공부 기록 노트</description></item><item><title/><link>https://yera10.github.io/docs/mystudy/hidden/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yera10.github.io/docs/mystudy/hidden/</guid><description>This page is hidden in menu # Quondam non pater est dignior ille Eurotas # Latent te facies # Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.
Pater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor Cum honorum Latona # O fallor in sustinui iussorum equidem.</description></item></channel></rss>